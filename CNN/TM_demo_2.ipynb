{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1670.05     0.       0.       0.       0.      68.585    0.       0.\n",
      "    36.36   -11.32   -88.09 ]\n",
      " [1670.06     0.       0.       0.       0.      68.585    0.       0.\n",
      "    36.36   -11.32   -88.08 ]\n",
      " [1670.07     0.       0.       0.       0.      68.585    0.       0.\n",
      "    36.36   -11.32   -88.07 ]] (19997, 11)\n",
      "[[880.11         0.           0.           0.           0.\n",
      "   67.8328125    0.           0.          36.51       -11.24690476\n",
      "  -89.22      ]\n",
      " [880.12         0.           0.           0.           0.\n",
      "   67.8328125    0.           0.          36.51       -11.24928571\n",
      "  -89.22      ]\n",
      " [880.13         0.           0.           0.           0.\n",
      "   67.8328125    0.           0.          36.51       -11.25\n",
      "  -89.22      ]] (15991, 11)\n"
     ]
    }
   ],
   "source": [
    "itxt=\"5.txt\"\n",
    "with open(itxt,\"r\") as f:\n",
    "    datalist = f.readlines()\n",
    "datanp=np.zeros((len(datalist),11))\n",
    "itxt1=\"7.txt\"\n",
    "with open(itxt1,\"r\") as f:\n",
    "    datalist1 = f.readlines()\n",
    "datanp1=np.zeros((len(datalist1),11))\n",
    "\n",
    "count=0\n",
    "for i in datalist:\n",
    "    datai=i.split(\"\\n\")[0].split(\"\\t\")\n",
    "    #print(datai)\n",
    "    for j in range(11):\n",
    "        #print(j)\n",
    "        datanp[count:count+1,j]=float(datai[j])\n",
    "    count=count+1\n",
    "print(datanp[0:3,:],datanp.shape)\n",
    "count1=0\n",
    "for i in datalist1:\n",
    "    datai=i.split(\"\\n\")[0].split(\"\\t\")\n",
    "    #print(datai)\n",
    "    for j in range(11):\n",
    "        #print(j)\n",
    "        datanp1[count1:count1+1,j]=float(datai[j])\n",
    "    count1=count1+1\n",
    "print(datanp1[0:3,:],datanp1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 10, 11) (1999, 11)\n",
      "(1599, 10, 11) (1599, 11)\n"
     ]
    }
   ],
   "source": [
    "train_data = datanp\n",
    "test_data = datanp1\n",
    "def get_XY(dat, time_steps):\n",
    "    # Indices of target array\n",
    "    Y_ind = np.arange(time_steps, dat.shape[0], time_steps)\n",
    "    Y = dat[Y_ind,:]\n",
    "    # Prepare X\n",
    "    rows_x = Y.shape[0]\n",
    "    X = dat[range(time_steps*rows_x),:]\n",
    "    X = np.reshape(X, (rows_x, time_steps, 11))    \n",
    "    return X, Y\n",
    "time_steps = 10\n",
    "trainX, trainY = get_XY(train_data, time_steps)\n",
    "testX, testY = get_XY(test_data, time_steps)\n",
    "print(trainX.shape,trainY.shape)\n",
    "print(testX.shape,testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=trainX[:,:,1:5]\n",
    "y_train=trainY[:,5:6]\n",
    "x_test=testX[:,:,1:5]\n",
    "y_test=testY[:,5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 10, 4)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 10, 4)       8           ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 10, 4)       19460       ['layer_normalization_24[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 10, 4)        0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 10, 4)       0           ['dropout_27[0][0]',             \n",
      " ambda)                                                           'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_24[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 10, 4)        20          ['layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 10, 4)        0           ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 10, 4)        20          ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 10, 4)       0           ['conv1d_25[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_25[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 10, 4)       19460       ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 10, 4)        0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 10, 4)       0           ['dropout_29[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_26[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 10, 4)        20          ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 10, 4)        0           ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 10, 4)        20          ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 10, 4)       0           ['conv1d_27[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_27[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 10, 4)       19460       ['layer_normalization_28[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 10, 4)        0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 10, 4)       0           ['dropout_31[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_28[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 10, 4)        20          ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 10, 4)        0           ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 10, 4)        20          ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 10, 4)       0           ['conv1d_29[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_29[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 10, 4)       19460       ['layer_normalization_30[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 10, 4)        0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 10, 4)       0           ['dropout_33[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 10, 4)       8           ['tf.__operators__.add_30[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 10, 4)        20          ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 10, 4)        0           ['conv1d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 10, 4)        20          ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 10, 4)       0           ['conv1d_31[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 10)          0           ['tf.__operators__.add_31[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          1408        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            129         ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 79,601\n",
      "Trainable params: 79,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "25/25 [==============================] - 23s 291ms/step - loss: 3271.0215 - val_loss: 1681.9806\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 7s 255ms/step - loss: 459.5820 - val_loss: 292.5316\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 7s 258ms/step - loss: 272.8012 - val_loss: 259.3167\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 6s 256ms/step - loss: 246.2550 - val_loss: 288.2499\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 7s 259ms/step - loss: 253.5022 - val_loss: 271.7468\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 232.5229 - val_loss: 270.1459\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 6s 248ms/step - loss: 231.9978 - val_loss: 298.7265\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 6s 254ms/step - loss: 229.0936 - val_loss: 284.7193\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 6s 255ms/step - loss: 225.2918 - val_loss: 293.7385\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 7s 260ms/step - loss: 225.3882 - val_loss: 265.9560\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 7s 257ms/step - loss: 215.5667 - val_loss: 290.7790\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 221.1402 - val_loss: 287.2595\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 6s 253ms/step - loss: 220.9757 - val_loss: 282.9087\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 6s 246ms/step - loss: 220.8881 - val_loss: 284.0203\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 6s 245ms/step - loss: 211.0207 - val_loss: 267.9646\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 6s 254ms/step - loss: 222.9667 - val_loss: 322.7733\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 7s 257ms/step - loss: 218.6041 - val_loss: 306.3165\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 7s 257ms/step - loss: 205.5089 - val_loss: 300.4731\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 7s 268ms/step - loss: 209.8518 - val_loss: 306.1154\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 209.5797 - val_loss: 288.2980\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 7s 271ms/step - loss: 212.3112 - val_loss: 289.5349\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 205.0090 - val_loss: 301.3622\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 207.3987 - val_loss: 287.3230\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 204.7761 - val_loss: 300.9271\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 196.6400 - val_loss: 288.5382\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 211.9868 - val_loss: 361.3658\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 7s 276ms/step - loss: 200.7876 - val_loss: 289.7595\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 7s 288ms/step - loss: 212.7587 - val_loss: 284.1550\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 201.6530 - val_loss: 305.9586\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 197.8919 - val_loss: 374.7964\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 209.0651 - val_loss: 308.2234\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 7s 264ms/step - loss: 194.4431 - val_loss: 307.5641\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 7s 270ms/step - loss: 203.5612 - val_loss: 300.8258\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 7s 270ms/step - loss: 208.4149 - val_loss: 327.1622\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 7s 269ms/step - loss: 202.0534 - val_loss: 322.3357\n",
      "Epoch 36/200\n",
      "25/25 [==============================] - 7s 283ms/step - loss: 202.0308 - val_loss: 351.3519\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 7s 289ms/step - loss: 196.6275 - val_loss: 335.4416\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 8s 328ms/step - loss: 209.4501 - val_loss: 337.3961\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 7s 278ms/step - loss: 210.4010 - val_loss: 323.3948\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 196.3714 - val_loss: 313.8514\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 207.5115 - val_loss: 333.6729\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 7s 289ms/step - loss: 200.7669 - val_loss: 382.9630\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 8s 315ms/step - loss: 198.9520 - val_loss: 341.3240\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 9s 344ms/step - loss: 198.0034 - val_loss: 385.9331\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 9s 362ms/step - loss: 199.5047 - val_loss: 328.5141\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 9s 366ms/step - loss: 203.4392 - val_loss: 371.7141\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 10s 420ms/step - loss: 210.8825 - val_loss: 353.3640\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 10s 401ms/step - loss: 204.2858 - val_loss: 366.9605\n",
      "Epoch 49/200\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 202.5125 - val_loss: 316.6481\n",
      "Epoch 50/200\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 195.5012 - val_loss: 334.0310\n",
      "Epoch 51/200\n",
      "25/25 [==============================] - 7s 275ms/step - loss: 202.6929 - val_loss: 328.2411\n",
      "Epoch 52/200\n",
      "25/25 [==============================] - 7s 263ms/step - loss: 190.2281 - val_loss: 367.2832\n",
      "Epoch 53/200\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 199.9158 - val_loss: 342.4328\n",
      "Epoch 54/200\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 191.9337 - val_loss: 418.6985\n",
      "Epoch 55/200\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 207.2088 - val_loss: 326.4499\n",
      "Epoch 56/200\n",
      "25/25 [==============================] - 7s 277ms/step - loss: 207.3410 - val_loss: 333.7086\n",
      "Epoch 57/200\n",
      "25/25 [==============================] - 7s 282ms/step - loss: 195.4146 - val_loss: 364.5271\n",
      "Epoch 58/200\n",
      "25/25 [==============================] - 7s 288ms/step - loss: 206.0668 - val_loss: 414.6057\n",
      "Epoch 59/200\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 197.0354 - val_loss: 374.6882\n",
      "Epoch 60/200\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 192.3248 - val_loss: 397.3278\n",
      "Epoch 61/200\n",
      "25/25 [==============================] - 6s 257ms/step - loss: 184.7609 - val_loss: 366.6400\n",
      "Epoch 62/200\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 195.9862 - val_loss: 348.4529\n",
      "Epoch 63/200\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 190.1547 - val_loss: 384.3556\n",
      "Epoch 64/200\n",
      "25/25 [==============================] - 7s 286ms/step - loss: 203.0062 - val_loss: 420.1391\n",
      "Epoch 65/200\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 201.4796 - val_loss: 362.0544\n",
      "Epoch 66/200\n",
      "25/25 [==============================] - 10s 403ms/step - loss: 197.1726 - val_loss: 385.2787\n",
      "Epoch 67/200\n",
      "25/25 [==============================] - 9s 382ms/step - loss: 194.3498 - val_loss: 391.6972\n",
      "Epoch 68/200\n",
      "25/25 [==============================] - 8s 328ms/step - loss: 198.8760 - val_loss: 409.6193\n",
      "Epoch 69/200\n",
      "25/25 [==============================] - 9s 350ms/step - loss: 207.2484 - val_loss: 430.1650\n",
      "Epoch 70/200\n",
      "25/25 [==============================] - 7s 283ms/step - loss: 195.8044 - val_loss: 336.3006\n",
      "Epoch 71/200\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 199.1128 - val_loss: 421.2938\n",
      "Epoch 72/200\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 198.6077 - val_loss: 370.1140\n",
      "Epoch 73/200\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 195.9809 - val_loss: 428.4220\n",
      "Epoch 74/200\n",
      "25/25 [==============================] - 10s 393ms/step - loss: 189.7659 - val_loss: 419.4897\n",
      "Epoch 75/200\n",
      "25/25 [==============================] - 9s 358ms/step - loss: 204.1800 - val_loss: 392.4374\n",
      "Epoch 76/200\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 189.2835 - val_loss: 339.4817\n",
      "Epoch 77/200\n",
      "25/25 [==============================] - 8s 333ms/step - loss: 199.3941 - val_loss: 400.2375\n",
      "Epoch 78/200\n",
      "25/25 [==============================] - 8s 318ms/step - loss: 199.6862 - val_loss: 362.1033\n",
      "Epoch 79/200\n",
      "25/25 [==============================] - 8s 321ms/step - loss: 191.8097 - val_loss: 400.4622\n",
      "Epoch 80/200\n",
      "25/25 [==============================] - 7s 288ms/step - loss: 198.3497 - val_loss: 315.6647\n",
      "Epoch 81/200\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 208.8056 - val_loss: 415.3639\n",
      "Epoch 82/200\n",
      "25/25 [==============================] - 9s 339ms/step - loss: 190.7178 - val_loss: 433.2889\n",
      "Epoch 83/200\n",
      "25/25 [==============================] - 9s 354ms/step - loss: 200.9854 - val_loss: 426.8176\n",
      "Epoch 84/200\n",
      "25/25 [==============================] - 9s 371ms/step - loss: 197.9252 - val_loss: 362.5681\n",
      "Epoch 85/200\n",
      "25/25 [==============================] - 8s 327ms/step - loss: 198.0368 - val_loss: 412.3167\n",
      "Epoch 86/200\n",
      "25/25 [==============================] - 9s 356ms/step - loss: 197.5053 - val_loss: 422.7005\n",
      "Epoch 87/200\n",
      "25/25 [==============================] - 8s 332ms/step - loss: 196.0215 - val_loss: 414.0447\n",
      "Epoch 88/200\n",
      "25/25 [==============================] - 7s 290ms/step - loss: 192.1408 - val_loss: 396.1461\n",
      "Epoch 89/200\n",
      "25/25 [==============================] - 10s 421ms/step - loss: 197.1771 - val_loss: 415.9489\n",
      "Epoch 90/200\n",
      "25/25 [==============================] - 9s 368ms/step - loss: 187.6413 - val_loss: 411.8669\n",
      "Epoch 91/200\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 194.5899 - val_loss: 433.0904\n",
      "Epoch 92/200\n",
      "25/25 [==============================] - 7s 270ms/step - loss: 196.1985 - val_loss: 366.0529\n",
      "Epoch 93/200\n",
      "25/25 [==============================] - 7s 268ms/step - loss: 195.9149 - val_loss: 358.9218\n",
      "Epoch 94/200\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 197.5431 - val_loss: 368.5577\n",
      "Epoch 95/200\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 204.5269 - val_loss: 363.0215\n",
      "Epoch 96/200\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 191.2891 - val_loss: 407.6143\n",
      "Epoch 97/200\n",
      "25/25 [==============================] - 7s 276ms/step - loss: 188.6355 - val_loss: 372.6582\n",
      "Epoch 98/200\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 193.3039 - val_loss: 449.5331\n",
      "Epoch 99/200\n",
      "25/25 [==============================] - 9s 346ms/step - loss: 195.6543 - val_loss: 344.9340\n",
      "Epoch 100/200\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 199.6961 - val_loss: 402.9760\n",
      "Epoch 101/200\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 192.6494 - val_loss: 449.1939\n",
      "Epoch 102/200\n",
      "25/25 [==============================] - 9s 364ms/step - loss: 184.8579 - val_loss: 430.7499\n",
      "Epoch 103/200\n",
      "25/25 [==============================] - 7s 270ms/step - loss: 189.4077 - val_loss: 380.1843\n",
      "Epoch 104/200\n",
      "25/25 [==============================] - 9s 342ms/step - loss: 190.2791 - val_loss: 379.1404\n",
      "Epoch 105/200\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 202.0325 - val_loss: 406.5964\n",
      "Epoch 106/200\n",
      "25/25 [==============================] - 8s 325ms/step - loss: 190.9069 - val_loss: 425.0681\n",
      "Epoch 107/200\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 191.0032 - val_loss: 449.0876\n",
      "Epoch 108/200\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 193.9559 - val_loss: 366.8265\n",
      "Epoch 109/200\n",
      "25/25 [==============================] - 10s 384ms/step - loss: 203.3715 - val_loss: 419.3157\n",
      "Epoch 110/200\n",
      "25/25 [==============================] - 9s 338ms/step - loss: 192.0812 - val_loss: 433.2507\n",
      "Epoch 111/200\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 187.1379 - val_loss: 436.5893\n",
      "Epoch 112/200\n",
      "25/25 [==============================] - 9s 351ms/step - loss: 193.9558 - val_loss: 444.5169\n",
      "Epoch 113/200\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 191.6063 - val_loss: 489.5980\n",
      "Epoch 114/200\n",
      "25/25 [==============================] - 8s 335ms/step - loss: 199.2302 - val_loss: 373.5827\n",
      "Epoch 115/200\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 206.9640 - val_loss: 397.1288\n",
      "Epoch 116/200\n",
      "25/25 [==============================] - 9s 369ms/step - loss: 190.3988 - val_loss: 435.8658\n",
      "Epoch 117/200\n",
      "25/25 [==============================] - 8s 326ms/step - loss: 191.4814 - val_loss: 390.8412\n",
      "Epoch 118/200\n",
      "25/25 [==============================] - 8s 335ms/step - loss: 194.8406 - val_loss: 423.9507\n",
      "Epoch 119/200\n",
      "25/25 [==============================] - 8s 334ms/step - loss: 197.0446 - val_loss: 480.6364\n",
      "Epoch 120/200\n",
      "25/25 [==============================] - 10s 399ms/step - loss: 198.2795 - val_loss: 396.0533\n",
      "Epoch 121/200\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 193.7515 - val_loss: 423.9054\n",
      "Epoch 122/200\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 193.5683 - val_loss: 433.5812\n",
      "Epoch 123/200\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 201.1179 - val_loss: 355.3259\n",
      "Epoch 124/200\n",
      "25/25 [==============================] - 7s 266ms/step - loss: 190.8848 - val_loss: 388.2200\n",
      "Epoch 125/200\n",
      "25/25 [==============================] - 7s 266ms/step - loss: 195.5121 - val_loss: 501.4734\n",
      "Epoch 126/200\n",
      "25/25 [==============================] - 7s 264ms/step - loss: 193.6530 - val_loss: 406.1793\n",
      "Epoch 127/200\n",
      "25/25 [==============================] - 7s 290ms/step - loss: 193.8906 - val_loss: 438.2248\n",
      "Epoch 128/200\n",
      "25/25 [==============================] - 7s 283ms/step - loss: 200.5600 - val_loss: 492.7262\n",
      "Epoch 129/200\n",
      "25/25 [==============================] - 7s 282ms/step - loss: 212.1212 - val_loss: 352.8408\n",
      "Epoch 130/200\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 190.6443 - val_loss: 437.2447\n",
      "Epoch 131/200\n",
      "25/25 [==============================] - 8s 325ms/step - loss: 192.6150 - val_loss: 435.2643\n",
      "Epoch 132/200\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 193.2313 - val_loss: 419.4457\n",
      "Epoch 133/200\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 196.5120 - val_loss: 394.1373\n",
      "Epoch 134/200\n",
      "25/25 [==============================] - 8s 318ms/step - loss: 195.9187 - val_loss: 383.0774\n",
      "Epoch 135/200\n",
      "25/25 [==============================] - 8s 330ms/step - loss: 197.2028 - val_loss: 448.2206\n",
      "Epoch 136/200\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 191.7393 - val_loss: 460.5422\n",
      "Epoch 137/200\n",
      "25/25 [==============================] - 7s 288ms/step - loss: 196.9549 - val_loss: 368.0161\n",
      "Epoch 138/200\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 191.1667 - val_loss: 418.8018\n",
      "Epoch 139/200\n",
      "25/25 [==============================] - 9s 351ms/step - loss: 190.3867 - val_loss: 370.5497\n",
      "Epoch 140/200\n",
      "25/25 [==============================] - 6s 246ms/step - loss: 188.7942 - val_loss: 439.7809\n",
      "Epoch 141/200\n",
      "25/25 [==============================] - 9s 371ms/step - loss: 194.5225 - val_loss: 381.5652\n",
      "Epoch 142/200\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 194.3347 - val_loss: 484.2291\n",
      "Epoch 143/200\n",
      "25/25 [==============================] - 7s 278ms/step - loss: 199.3768 - val_loss: 368.8179\n",
      "Epoch 144/200\n",
      "25/25 [==============================] - 7s 300ms/step - loss: 201.7648 - val_loss: 387.5939\n",
      "Epoch 145/200\n",
      "25/25 [==============================] - 7s 290ms/step - loss: 189.0490 - val_loss: 415.4514\n",
      "Epoch 146/200\n",
      "25/25 [==============================] - 7s 270ms/step - loss: 200.0443 - val_loss: 429.9008\n",
      "Epoch 147/200\n",
      "25/25 [==============================] - 6s 253ms/step - loss: 195.3654 - val_loss: 421.7115\n",
      "Epoch 148/200\n",
      "25/25 [==============================] - 7s 280ms/step - loss: 191.7886 - val_loss: 398.4828\n",
      "Epoch 149/200\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 193.5684 - val_loss: 432.8464\n",
      "Epoch 150/200\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 192.6086 - val_loss: 402.9718\n",
      "Epoch 151/200\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 196.1456 - val_loss: 487.6067\n",
      "Epoch 152/200\n",
      "25/25 [==============================] - 7s 285ms/step - loss: 191.4471 - val_loss: 420.4204\n",
      "Epoch 153/200\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 185.3576 - val_loss: 444.2418\n",
      "Epoch 154/200\n",
      "25/25 [==============================] - 8s 331ms/step - loss: 195.8277 - val_loss: 420.3245\n",
      "Epoch 155/200\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 199.1585 - val_loss: 387.9900\n",
      "Epoch 156/200\n",
      "25/25 [==============================] - 7s 263ms/step - loss: 187.8951 - val_loss: 412.9237\n",
      "Epoch 157/200\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 183.4739 - val_loss: 465.2987\n",
      "Epoch 158/200\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 186.7882 - val_loss: 464.8831\n",
      "Epoch 159/200\n",
      "25/25 [==============================] - 8s 329ms/step - loss: 188.9191 - val_loss: 402.4689\n",
      "Epoch 160/200\n",
      "25/25 [==============================] - 7s 285ms/step - loss: 190.1243 - val_loss: 384.2025\n",
      "Epoch 161/200\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 197.3238 - val_loss: 428.6345\n",
      "Epoch 162/200\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 184.4689 - val_loss: 474.3605\n",
      "Epoch 163/200\n",
      "25/25 [==============================] - 8s 321ms/step - loss: 195.6671 - val_loss: 450.7301\n",
      "Epoch 164/200\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 194.4921 - val_loss: 373.9383\n",
      "Epoch 165/200\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 198.1411 - val_loss: 497.0700\n",
      "Epoch 166/200\n",
      "25/25 [==============================] - 8s 331ms/step - loss: 191.4897 - val_loss: 332.1432\n",
      "Epoch 167/200\n",
      "25/25 [==============================] - 9s 365ms/step - loss: 197.6649 - val_loss: 414.6990\n",
      "Epoch 168/200\n",
      "25/25 [==============================] - 9s 369ms/step - loss: 188.3455 - val_loss: 446.1376\n",
      "Epoch 169/200\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 190.7753 - val_loss: 433.9349\n",
      "Epoch 170/200\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 197.9510 - val_loss: 408.3907\n",
      "Epoch 171/200\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 186.5466 - val_loss: 449.5878\n",
      "Epoch 172/200\n",
      "25/25 [==============================] - 10s 380ms/step - loss: 199.1906 - val_loss: 399.4111\n",
      "Epoch 173/200\n",
      "25/25 [==============================] - 8s 325ms/step - loss: 197.0874 - val_loss: 471.0252\n",
      "Epoch 174/200\n",
      "25/25 [==============================] - 8s 332ms/step - loss: 193.4023 - val_loss: 385.1160\n",
      "Epoch 175/200\n",
      "25/25 [==============================] - 9s 338ms/step - loss: 191.7229 - val_loss: 475.0607\n",
      "Epoch 176/200\n",
      "25/25 [==============================] - 9s 337ms/step - loss: 198.4246 - val_loss: 383.7329\n",
      "Epoch 177/200\n",
      "25/25 [==============================] - 8s 322ms/step - loss: 197.1383 - val_loss: 406.8864\n",
      "Epoch 178/200\n",
      "25/25 [==============================] - 8s 326ms/step - loss: 199.6202 - val_loss: 415.9113\n",
      "Epoch 179/200\n",
      "25/25 [==============================] - 9s 354ms/step - loss: 189.6806 - val_loss: 410.5509\n",
      "Epoch 180/200\n",
      "25/25 [==============================] - 9s 347ms/step - loss: 192.2422 - val_loss: 449.1859\n",
      "Epoch 181/200\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 194.5481 - val_loss: 424.3145\n",
      "Epoch 182/200\n",
      "25/25 [==============================] - 7s 276ms/step - loss: 193.1804 - val_loss: 441.4278\n",
      "Epoch 183/200\n",
      "25/25 [==============================] - 8s 336ms/step - loss: 188.9724 - val_loss: 417.5728\n",
      "Epoch 184/200\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 180.7068 - val_loss: 417.4103\n",
      "Epoch 185/200\n",
      "25/25 [==============================] - 9s 345ms/step - loss: 187.5435 - val_loss: 489.8958\n",
      "Epoch 186/200\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 200.9921 - val_loss: 423.1365\n",
      "Epoch 187/200\n",
      "25/25 [==============================] - 8s 316ms/step - loss: 194.1551 - val_loss: 456.5229\n",
      "Epoch 188/200\n",
      "25/25 [==============================] - 8s 299ms/step - loss: 192.7383 - val_loss: 408.9127\n",
      "Epoch 189/200\n",
      "25/25 [==============================] - 9s 344ms/step - loss: 195.5873 - val_loss: 508.7925\n",
      "Epoch 190/200\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 200.5579 - val_loss: 457.9098\n",
      "Epoch 191/200\n",
      "25/25 [==============================] - 6s 249ms/step - loss: 187.2166 - val_loss: 383.6027\n",
      "Epoch 192/200\n",
      "25/25 [==============================] - 7s 279ms/step - loss: 194.3049 - val_loss: 493.3111\n",
      "Epoch 193/200\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 195.4381 - val_loss: 419.9951\n",
      "Epoch 194/200\n",
      "25/25 [==============================] - 7s 285ms/step - loss: 187.7143 - val_loss: 399.9120\n",
      "Epoch 195/200\n",
      "25/25 [==============================] - 8s 337ms/step - loss: 187.5952 - val_loss: 402.4673\n",
      "Epoch 196/200\n",
      "25/25 [==============================] - 6s 249ms/step - loss: 181.6122 - val_loss: 446.5558\n",
      "Epoch 197/200\n",
      "25/25 [==============================] - 6s 244ms/step - loss: 193.7791 - val_loss: 458.0885\n",
      "Epoch 198/200\n",
      "25/25 [==============================] - 7s 264ms/step - loss: 176.0521 - val_loss: 443.2939\n",
      "Epoch 199/200\n",
      "25/25 [==============================] - 7s 269ms/step - loss: 193.5512 - val_loss: 431.0834\n",
      "Epoch 200/200\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 185.6603 - val_loss: 443.1973\n",
      "50/50 [==============================] - 4s 75ms/step - loss: 283.1919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283.1919250488281"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    #callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
